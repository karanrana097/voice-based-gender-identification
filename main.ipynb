{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3811e7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/karan/anaconda3/lib/python3.11/site-packages (2.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de2b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b22190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1ae880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/cv-other-train/sample-069205.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/cv-valid-train/sample-063134.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/cv-other-train/sample-080873.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/cv-other-train/sample-105595.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/cv-valid-train/sample-144613.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  gender\n",
       "0  data/cv-other-train/sample-069205.npy  female\n",
       "1  data/cv-valid-train/sample-063134.npy  female\n",
       "2  data/cv-other-train/sample-080873.npy  female\n",
       "3  data/cv-other-train/sample-105595.npy  female\n",
       "4  data/cv-valid-train/sample-144613.npy  female"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"balanced-all.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee13a620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66933</th>\n",
       "      <td>data/cv-valid-train/sample-171098.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66934</th>\n",
       "      <td>data/cv-other-train/sample-022864.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66935</th>\n",
       "      <td>data/cv-valid-train/sample-080933.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66936</th>\n",
       "      <td>data/cv-other-train/sample-012026.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66937</th>\n",
       "      <td>data/cv-other-train/sample-013841.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename gender\n",
       "66933  data/cv-valid-train/sample-171098.npy   male\n",
       "66934  data/cv-other-train/sample-022864.npy   male\n",
       "66935  data/cv-valid-train/sample-080933.npy   male\n",
       "66936  data/cv-other-train/sample-012026.npy   male\n",
       "66937  data/cv-other-train/sample-013841.npy   male"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9029aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    }
   ],
   "source": [
    "n_sample=len(df)\n",
    "n_male_sample=len(df[df['gender']=='male'])\n",
    "n_female_sample=len(df[df['gender']=='female'])\n",
    "print(\"total samples :\",n_sample)\n",
    "print(\"total male sample:\",n_male_sample)\n",
    "print(\"total female sample:\",n_female_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25614a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b752627",
   "metadata": {},
   "outputs": [],
   "source": [
    "lablel2int={\n",
    "    \"male\":1,\n",
    "    \"female\":0\n",
    "}\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/lables.npy\"):\n",
    "        X=np.load(\"results/features.npy\")\n",
    "        y=np.load(\"results/labels.npy\")\n",
    "        return X,y\n",
    "    df=pd.read_csv(\"balanced-all.csv\")\n",
    "    n_sample=len(df)\n",
    "    n_male_sample=len(df[df['gender']=='male'])\n",
    "    n_female_sample=len(df[df['gender']=='female'])\n",
    "    print(\"total samples :\",n_sample)\n",
    "    print(\"total male sample:\",n_male_sample)\n",
    "    print(\"total female sample:\",n_female_sample)\n",
    "    \n",
    "    X=np.zeros((n_sample, vector_length))\n",
    "    y=np.zeros((n_sample,1))\n",
    "    \n",
    "    for i,(filename,gender) in tqdm.tqdm(enumerate(zip(df['filename'],df['gender'])),\"Loding data\",total=n_sample):\n",
    "        features=np.load(filename)\n",
    "        X[i]=features\n",
    "        y[i]=lablel2int[gender]\n",
    "        \n",
    "    np.save(\"results/features\",X)\n",
    "    np.save(\"results/labels\",y)\n",
    "    return X,y\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67cd2f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loding data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66938/66938 [00:13<00:00, 4958.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[8.33691835e-01, 1.84069288e+00, 7.21150815e-01, ...,\n",
       "         2.22568936e-03, 3.72419163e-04, 4.14350980e-05],\n",
       "        [3.44792323e-04, 3.78249679e-04, 7.02428690e-04, ...,\n",
       "         5.25400345e-08, 2.83253740e-08, 2.14376583e-09],\n",
       "        [1.90136401e-04, 2.43997492e-04, 4.60427953e-04, ...,\n",
       "         1.14822782e-04, 2.32082821e-05, 1.38619748e-06],\n",
       "        ...,\n",
       "        [1.16413343e-03, 9.43325169e-04, 1.52071775e-03, ...,\n",
       "         3.66348862e-09, 1.36717937e-09, 1.47780455e-10],\n",
       "        [6.36986643e-02, 1.34973586e-01, 2.42811322e+00, ...,\n",
       "         3.79042875e-04, 8.72957244e-05, 6.89193166e-06],\n",
       "        [1.45035911e+00, 1.17254317e+00, 2.72492021e-01, ...,\n",
       "         2.80375849e-03, 1.14007434e-03, 1.97923204e-04]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276bc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c91ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,test_size=0.1,valid_size=0.1):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_size,random_state=7)\n",
    "    X_train,X_valid,y_train,y_valid=train_test_split(X_train,y_train,test_size=valid_size,random_state=7)\n",
    "    return{\n",
    "        \"X_train\":X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb6129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loding data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66938/66938 [00:13<00:00, 4973.49it/s]\n"
     ]
    }
   ],
   "source": [
    "X,y=load_data()\n",
    "data=split_data(X,y,test_size=0.1,valid_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e2ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b608b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vector_length=128):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(256,input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"],optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205825 (804.00 KB)\n",
      "Trainable params: 205825 (804.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1513d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666fb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2315 - accuracy: 0.9141 - val_loss: 0.2084 - val_accuracy: 0.9250\n",
      "Epoch 2/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2202 - accuracy: 0.9177 - val_loss: 0.2071 - val_accuracy: 0.9258\n",
      "Epoch 3/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2217 - accuracy: 0.9178 - val_loss: 0.2066 - val_accuracy: 0.9251\n",
      "Epoch 4/120\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.2175 - accuracy: 0.9198 - val_loss: 0.2048 - val_accuracy: 0.9266\n",
      "Epoch 5/120\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.2142 - accuracy: 0.9208 - val_loss: 0.2050 - val_accuracy: 0.9265\n",
      "Epoch 6/120\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2144 - accuracy: 0.9205 - val_loss: 0.2039 - val_accuracy: 0.9265\n",
      "Epoch 7/120\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2151 - accuracy: 0.9205 - val_loss: 0.2030 - val_accuracy: 0.9273\n",
      "Epoch 8/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2118 - accuracy: 0.9211 - val_loss: 0.2044 - val_accuracy: 0.9232\n",
      "Epoch 9/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2136 - accuracy: 0.9200 - val_loss: 0.2025 - val_accuracy: 0.9263\n",
      "Epoch 10/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2125 - accuracy: 0.9218 - val_loss: 0.2018 - val_accuracy: 0.9261\n",
      "Epoch 11/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2091 - accuracy: 0.9221 - val_loss: 0.2009 - val_accuracy: 0.9265\n",
      "Epoch 12/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2069 - accuracy: 0.9222 - val_loss: 0.2001 - val_accuracy: 0.9256\n",
      "Epoch 13/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2083 - accuracy: 0.9225 - val_loss: 0.2021 - val_accuracy: 0.9270\n",
      "Epoch 14/120\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2078 - accuracy: 0.9218 - val_loss: 0.2008 - val_accuracy: 0.9260\n",
      "Epoch 15/120\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2088 - accuracy: 0.9225 - val_loss: 0.2022 - val_accuracy: 0.9250\n",
      "Epoch 16/120\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2070 - accuracy: 0.9222 - val_loss: 0.2013 - val_accuracy: 0.9268\n",
      "Epoch 17/120\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.2097 - accuracy: 0.9223 - val_loss: 0.2010 - val_accuracy: 0.9291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2982f58d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir=\"logs\")\n",
    "early_stopping=EarlyStopping(mode=\"min\",patience=5,restore_best_weights=True)\n",
    "\n",
    "batch_size=1024\n",
    "\n",
    "epochs=120\n",
    "\n",
    "model.fit(data[\"X_train\"],data[\"y_train\"],epochs=epochs,batch_size=batch_size,validation_data=(data[\"X_valid\"],data[\"y_valid\"]),\n",
    "         callbacks=[tensorboard,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e71c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riya\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the native Keras format\n",
    "model.save(\"results/model.keras\")\n",
    "model.save(\"results/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6d1478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model using 6694 samples...\n",
      "210/210 [==============================] - 0s 805us/step - loss: 0.1983 - accuracy: 0.9265\n",
      "Loss:0.1983\n",
      "Accuracy: 92.65%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss,accuracy=model.evaluate(data[\"X_test\"],data[\"y_test\"],verbose=1)\n",
    "\n",
    "print(f\"Loss:{loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26242cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librosa is installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import librosa\n",
    "    print(\"librosa is installed.\")\n",
    "except ImportError:\n",
    "    print(\"librosa is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ddd44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Obtaining dependency information for librosa from https://files.pythonhosted.org/packages/e2/a2/4f639c1168d7aada749a896afb4892a831e2041bebdcf636aebfe9e86556/librosa-0.10.1-py3-none-any.whl.metadata\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (0.57.0)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pooch>=1.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (1.4.0)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Obtaining dependency information for soxr>=0.3.2 from https://files.pythonhosted.org/packages/13/6b/c162457850c2e7db02a71db1ca1933aa2518cfac0f5524a6b8f93d737475/soxr-0.3.6-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading soxr-0.3.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (0.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.40.0)\n",
      "Requirement already satisfied: requests in /Users/karan/anaconda3/lib/python3.11/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: packaging in /Users/karan/anaconda3/lib/python3.11/site-packages (from pooch>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: appdirs in /Users/karan/anaconda3/lib/python3.11/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/karan/anaconda3/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/karan/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/karan/anaconda3/lib/python3.11/site-packages (from requests->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/karan/anaconda3/lib/python3.11/site-packages (from requests->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karan/anaconda3/lib/python3.11/site-packages (from requests->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/karan/anaconda3/lib/python3.11/site-packages (from requests->pooch>=1.0->librosa) (2023.7.22)\n",
      "Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.3.6-cp311-cp311-macosx_11_0_arm64.whl (385 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23704 sha256=14d13e432bbce319bb4031aa3bb0b63c8e5b30f357f116d86f2d6e467c715242\n",
      "  Stored in directory: /Users/karan/Library/Caches/pip/wheels/34/6d/02/40eff8045c8287806e7d83333301c1c7bab6109045424e8558\n",
      "Successfully built audioread\n",
      "Installing collected packages: soxr, audioread, soundfile, librosa\n",
      "Successfully installed audioread-3.0.0 librosa-0.10.1 soundfile-0.12.1 soxr-0.3.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9af65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    \n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d95eda1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyaudio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwave\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f36be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone...\n",
      "Recording completed.\n",
      "Saved recorded voice to 'recorded_voice.wav'.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "OUTPUT_FILE = \"recorded_voice.wav\"\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording started. Speak into the microphone...\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Record audio for the specified duration\n",
    "for i in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"Recording completed.\")\n",
    "\n",
    "# Stop and close the audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded audio to a WAV file\n",
    "with wave.open(OUTPUT_FILE, \"wb\") as wf:\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "print(f\"Saved recorded voice to '{OUTPUT_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5292c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 281ms/step\n",
      "Result: male\n",
      "Probabilities:     Male: 88.72%    Female: 11.28%\n"
     ]
    }
   ],
   "source": [
    "file_name=\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\n",
    "features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities:     Male: {male_prob * 100:.2f}%    Female: {female_prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa471864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.13.tar.gz (46 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/pyaudio\n",
      "  \u001b[31m   \u001b[0m copying src/pyaudio/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/pyaudio\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'pyaudio._portaudio' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/src/pyaudio\n",
      "  \u001b[31m   \u001b[0m clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/karan/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/karan/anaconda3/include -arch arm64 -DMACOS=1 -I/usr/local/include -I/usr/include -I/opt/homebrew/include -I/Users/karan/anaconda3/include/python3.11 -c src/pyaudio/device_api.c -o build/temp.macosx-11.1-arm64-cpython-311/src/pyaudio/device_api.o\n",
      "  \u001b[31m   \u001b[0m src/pyaudio/device_api.c:9:10: fatal error: 'portaudio.h' file not found\n",
      "  \u001b[31m   \u001b[0m #include \"portaudio.h\"\n",
      "  \u001b[31m   \u001b[0m          ^~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m 1 error generated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build pyaudio\n",
      "\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cde8891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan/anaconda3/lib/python3.11/tkinter/__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4q/mg4rsw2n1q99ng6nyk4wvp9h0000gn/T/ipykernel_28055/3471296818.py\", line 22, in record\n",
      "    FORMAT = pyaudio.paInt16\n",
      "             ^^^^^^^\n",
      "NameError: name 'pyaudio' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan/anaconda3/lib/python3.11/tkinter/__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4q/mg4rsw2n1q99ng6nyk4wvp9h0000gn/T/ipykernel_28055/3471296818.py\", line 22, in record\n",
      "    FORMAT = pyaudio.paInt16\n",
      "             ^^^^^^^\n",
      "NameError: name 'pyaudio' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan/anaconda3/lib/python3.11/tkinter/__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4q/mg4rsw2n1q99ng6nyk4wvp9h0000gn/T/ipykernel_28055/3471296818.py\", line 22, in record\n",
      "    FORMAT = pyaudio.paInt16\n",
      "             ^^^^^^^\n",
      "NameError: name 'pyaudio' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan/anaconda3/lib/python3.11/tkinter/__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4q/mg4rsw2n1q99ng6nyk4wvp9h0000gn/T/ipykernel_28055/3471296818.py\", line 22, in record\n",
      "    FORMAT = pyaudio.paInt16\n",
      "             ^^^^^^^\n",
      "NameError: name 'pyaudio' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan/anaconda3/lib/python3.11/tkinter/__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4q/mg4rsw2n1q99ng6nyk4wvp9h0000gn/T/ipykernel_28055/3471296818.py\", line 22, in record\n",
      "    FORMAT = pyaudio.paInt16\n",
      "             ^^^^^^^\n",
      "NameError: name 'pyaudio' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karan/anaconda3/lib/python3.11/tkinter/__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/4q/mg4rsw2n1q99ng6nyk4wvp9h0000gn/T/ipykernel_28055/3471296818.py\", line 22, in record\n",
      "    FORMAT = pyaudio.paInt16\n",
      "             ^^^^^^^\n",
      "NameError: name 'pyaudio' is not defined\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "from PIL import ImageTk,Image\n",
    "root=Tk()\n",
    "\n",
    "\n",
    "root.title(\"Voice-based gender detection\")\n",
    "root.geometry(\"600x400\")\n",
    "root.minsize(300,300)\n",
    "root.maxsize(700,700)\n",
    "\n",
    "root.iconbitmap(\"D:\\\\tkinter\\\\logo.ico\")\n",
    "root.configure(bg=\"lightblue\")\n",
    "\n",
    "header=Label(root,text=\"Voice based gender identification\",bg=\"lightblue\",\n",
    "            foreground=\"black\",font=(\"Arial\",24,\"bold\"))\n",
    "header.pack(pady=10)\n",
    "# root.wm_attributes('-transparentcolor',\"purple\")\n",
    "\n",
    "def record():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    RECORD_SECONDS = 5\n",
    "    OUTPUT_FILE = \"recorded_voice.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open the audio stream\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording started. Speak into the microphone...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Record audio for the specified duration\n",
    "    for i in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording completed.\")\n",
    "\n",
    "    # Stop and close the audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio to a WAV file\n",
    "    with wave.open(OUTPUT_FILE, \"wb\") as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "    print(f\"Saved recorded voice to '{OUTPUT_FILE}'.\")\n",
    "button1=Button(text=\"Record voice\",bg=\"lightblue\",activebackground=\"blue\",font=(\"Arial\",18,\"bold\"),\n",
    "      borderwidth=3,command=record)\n",
    "button1.pack(pady=30)\n",
    "predict_label=Label(root,text=\"\",bg=\"lightblue\",font=(\"Arial\",15,\"bold\"))\n",
    "predict_label.pack()\n",
    "\n",
    "def predict_gender():\n",
    "    file_name=\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\n",
    "    features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
    "    male_prob = model.predict(features)[0][0]\n",
    "    female_prob = 1 - male_prob\n",
    "    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "    # show the result!\n",
    "    print(\"Result:\", gender)\n",
    "    print(f\"Probabilities:     Male: {male_prob * 100:.2f}%    Female: {female_prob * 100:.2f}%\")\n",
    "    predict_label.config(text=\"Predicted Gender :\"+gender)\n",
    "\n",
    "button2=Button(text=\"Gender\",bg=\"lightblue\",activebackground=\"blue\",font=(\"Arial\",18,\"bold\"),\n",
    "      borderwidth=3,command=predict_gender)\n",
    "button2.pack(pady=25)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c2be516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02ff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645795bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
